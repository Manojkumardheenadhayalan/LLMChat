{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Browsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Full Browsing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "from aiohttp import ClientSession\n",
    "from lxml import html\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fastapi.concurrency import run_in_threadpool\n",
    "\n",
    "from app.shared import Shared\n",
    "from app.utils.chat.chains import Chains\n",
    "from app.common.constants import QueryBasedSearchTemplates, QueryTemplates\n",
    "from app.utils.logger import CustomLogger, LoggingConfig\n",
    "\n",
    "logger = CustomLogger(\n",
    "    \"Browsing\", logging_config=LoggingConfig(file_log_name=\"./logs/notebook.log\")\n",
    ")\n",
    "\n",
    "FEEDBACK_CLICK_OR_NOT_TEMPLATE: PromptTemplate = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a feedback bot that determines if the provided CONTEXT is sufficient to \"\n",
    "        \"answer the user's question. Follow the rules below to output a response.\\n- Outpu\"\n",
    "        't your next action to do in JSON form like {\"action\": YOUR_NEXT_ACTION, \"link\": '\n",
    "        'LINK_TO_CLICK}.\\n- \"action\" should be one of \"click\", \"finish\".\\n- {\"action\": \"cli'\n",
    "        'ck\"} should be selected when you want to click on a link to read more about it.\\n'\n",
    "        '- {\"action\": \"finish\"} should be selected when the information already provided '\n",
    "        'is sufficient to answer the user.\\n- \"link\" should be a link to click. You don\\'t '\n",
    "        'have to output \"link\" if you decided to take \"action\" as \"finish\".\\n- CONTEXT con'\n",
    "        \"sists of multiple #[LINK]\\\\n```TITLE\\\\nSNIPPET\\n```CONTEXT\\n{{context}}\\n```\\n```USER'\"\n",
    "        \"S QUESTION\\n{{query}}\\n```\"\n",
    "    ),\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template_format=\"jinja2\",\n",
    ")\n",
    "FEEDBACK_SUFFICIENT_OR_NOT_TEMPLATE: PromptTemplate = PromptTemplate(\n",
    "    template=(\n",
    "        \"You are a feedback bot that uses the context provided to determine if you can an\"\n",
    "        \"swer the user's question. Follow the rules below to output a response.\\n- Output \"\n",
    "        'your next action to do in JSON format like {\"is_sufficient\": TRUE_OR_FALSE}.\\n- \"'\n",
    "        'is_sufficient\" should be one of true or false.\\n```CONTEXT\\n{{context}}\\n```\\n```USE'\n",
    "        \"R'S QUESTION\\n{{query}}\\n```\"\n",
    "    ),\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template_format=\"jinja2\",\n",
    ")\n",
    "\n",
    "ddg = Shared().duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple vision pro\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain to me what the vision pro is that Apple released.\"\n",
    "query_to_search = await Chains._aget_query_to_search(\n",
    "    query=query, query_template=QueryBasedSearchTemplates.QUERY__JSONIFY_WEB_BROWSING\n",
    ")\n",
    "if query_to_search is None:\n",
    "    query_to_search = query\n",
    "    warn(f\"No query template found for query: {query}\")\n",
    "print(query_to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrolling(\n",
    "    link: str,\n",
    "    tokens_per_chunk: int,\n",
    "    chunk_overlap: int,\n",
    ") -> str | None:\n",
    "    async with ClientSession() as session:\n",
    "        res = await session.get(link)\n",
    "        paragraphs = html.fromstring(await res.text()).xpath(\"//p\")\n",
    "        scrollable_contents: list[str] = Shared().token_text_splitter.split_text(\n",
    "            \"\\n\".join([p.text_content().strip() for p in paragraphs]),\n",
    "            tokens_per_chunk=tokens_per_chunk,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        for scrollable_content in scrollable_contents:\n",
    "            scrollable_content = scrollable_content.strip()\n",
    "            logger.info(f\"Reading content: {scrollable_content}\")\n",
    "            sufficient_or_not_json = await Chains.aget_json(\n",
    "                query_template=FEEDBACK_SUFFICIENT_OR_NOT_TEMPLATE,\n",
    "                context=scrollable_content,\n",
    "                question=query,\n",
    "            )\n",
    "            logger.info(f\"sufficient_or_not_json: {sufficient_or_not_json}\")\n",
    "            if not isinstance(\n",
    "                sufficient_or_not_json, dict\n",
    "            ) or sufficient_or_not_json.get(\"is_sufficient\") not in (True, False):\n",
    "                logger.info(\"Reading content failed.\")\n",
    "                continue\n",
    "            if sufficient_or_not_json[\"is_sufficient\"]:\n",
    "                logger.info(\"Feedback bot decided to finish browsing.\")\n",
    "                return scrollable_content\n",
    "            else:\n",
    "                continue\n",
    "        logger.info(\"This link is not sufficient to answer the user's question.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def full_browsing(\n",
    "    query: str,\n",
    "    tokens_per_chunk: int,\n",
    "    chunk_overlap: int,\n",
    ") -> str | None:\n",
    "    query_to_search: str | None = await Chains._aget_query_to_search(\n",
    "        query=query,\n",
    "        query_template=QueryBasedSearchTemplates.QUERY__JSONIFY_WEB_BROWSING,\n",
    "    )\n",
    "    if query_to_search is None:\n",
    "        query_to_search = query\n",
    "\n",
    "    snippets_with_link: dict[str, str] = await run_in_threadpool(\n",
    "        ddg.formatted_results_with_link, query=query_to_search\n",
    "    )\n",
    "\n",
    "    while snippets_with_link:\n",
    "        action_and_link_json = await Chains.aget_json(\n",
    "            query_template=FEEDBACK_CLICK_OR_NOT_TEMPLATE,\n",
    "            query=query,\n",
    "            context=\"\\n\\n\".join(snippets_with_link.values()),\n",
    "        )\n",
    "        logger.info(f\"action_and_link_json: {action_and_link_json}\")\n",
    "        if (\n",
    "            not isinstance(action_and_link_json, dict)\n",
    "            or (action_and_link_json.get(\"action\") not in (\"click\", \"finish\"))\n",
    "            or (\n",
    "                action_and_link_json.get(\"action\") == \"link\"\n",
    "                and action_and_link_json.get(\"link\") not in snippets_with_link\n",
    "            )\n",
    "        ):\n",
    "            logger.info(\"Failed browsing.\")\n",
    "            return None\n",
    "        if action_and_link_json.get(\"action\") == \"finish\":\n",
    "            logger.info(\"We've got the answer!\")\n",
    "            return \"\\n\\n\".join(snippets_with_link.values())\n",
    "        snippets_with_link.pop(action_and_link_json[\"link\"])\n",
    "        scroll_result: str | None = await scrolling(\n",
    "            link=action_and_link_json[\"link\"],\n",
    "            tokens_per_chunk=tokens_per_chunk,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        if scroll_result is not None:\n",
    "            logger.info(\"We've got the answer!\")\n",
    "            return scroll_result\n",
    "        else:\n",
    "            logger.info(\"We still don't have the answer.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-13 21:20:35,213] Browsing:INFO - action_and_link_json: {'action': 'click', 'link': 'https://www.techradar.com/news/apple-vision-pro-everything-we-know'}\n",
      "[2023-06-13 21:20:35,760] Browsing:INFO - Reading content: When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.\n",
      "The Apple Vision Pro AR and VR headset is a reality\n",
      "After years of rumors, leaks, and speculation, Apple has finally unveiled the Vision Pro, its first AR and VR headset.\n",
      "-Mixed reality headset\n",
      "-Dual M2 and R1 chip setup\n",
      "-4K resolution per eye\n",
      "-No controllers, uses hand tracking and voice inputs\n",
      "-External battery pack\n",
      "-Two-hour battery life\n",
      "-Starts at $3,499 (around £2,800 / AU$5,300)\n",
      "-Runs on visionOS\n",
      "The announcement came as the 'One more thing' of Apple's WWDC 2023 event, at which it also unveiled a new 15-inch MacBook Air, new features coming with iOS 17, and a new M2 Ultra processor, among other reveals.\n",
      "Many of the leaks we'd heard proved to be correct, although a fair few weren't 100% accurate. So even if you've been following along for the past few months years we'd recommend reading on to get caught up on all the official info. Below you'll find all of the official details about the Apple Vision Pro VR headset, including its price, specs, design, and the vague release date Apple has provided.\n",
      "Vision Pro release date: Sometime \"early next year\" according to Apple.\n",
      "Vision Pro headset price: Starts at $3,499 (around £2,800 / AU$5,300).\n",
      "Vision Pro headset specs: Apple's headset uses two chipsets, an M2 and a new R1 to handle regular software and its XR capabilities respectively. It also has dual 4K displays.\n",
      "Vision Pro headset design: The Vision Pro has a similar design to other VR headsets, with a front panel that covers your eyes, and an elastic strap. One change from the norm is that it has an outer display to show the wearer's eyes.\n",
      "Vision Pro headset battery life: It lasts for up to two hours on a full charge using the official external battery pack.\n",
      "Vision Pro headset controllers: There are no controllers – instead you'll use your eyes, hands, and voice to control its visionOS software.\n",
      "Apple says the Vision Pro will \"start\" at $3,499 (that's around £2,800 / AU$5,300). That wording suggests that more expensive options will be available, but right now we don't know what those higher-priced headsets might offer over the standard model.\n",
      "As for release date for the Vision Pro, Apple has only given a vague “early next year.” That's later than we'd been expecting, with leaks suggesting it would launch in the next few months – perhaps around the same time as the iPhone 15 – but that isn't the case. As 2024 gets closer we expect Apple will give us an update on when we'll be able to strap a Vision Pro onto our heads.\n",
      "Interestingly, Apple's website only mentions a US release. Apple has yet to confirm if the Vision Pro will launch in regions outside of the US, and when that'll happen.\n",
      "The Apple Vision shares a lot of similarities with the current crop of best VR headsets. It has a large face panel that covers your eyes, and is secured to your head with a strap made from elasticated fabric, plastic and padding.\n",
      "But rather than the similarities, let's focus on the Vision Pro's unique design features.\n",
      "The biggest difference VR veterans will notice is that the Vision Pro doesn't have a battery; instead, it relies on an external battery pack. This is a sort of evolution of the HTC Vive XR Elite's design, which allowed the headset to go from being a headset with a battery in its strap to a battery-less pair of glasses that relies on external power.\n",
      "\n",
      "This battery pack will provide roughly two hours of use on a full charge according to Apple, and is small enough to fit in the wearer's pocket. It'll connect to the headset via a cable, which is a tad unseemly by Apple’s usual design standards, but what this choice lacks in style it should make up for in comfort. We found the Meta Quest Pro to be really comfy, but wearing it for extended periods of time can put a strain on your neck – just ask our writer who wore the Quest Pro for work for a whole week.\n",
      "If you buy a Vision Pro you'll find that your box lacks something needed for other VR headsets: controllers. That's because the Vision Pro relies solely on tracking your hand and eye movements, as well as voice inputs, to control its apps and experiences. It'll pick up these inputs using its array of 12 cameras, five sensors, and six microphones.\n",
      "The last design detail of note is the Vision Pro's Eyesight display. It looks pretty odd, maybe even a bit creepy, but we're reserving judgment until we've had a chance to try it out.\n",
      "\n",
      "When a Vision Pro wearer is using AR features and can see the real world, nearby people will see their eyes 'through' the headset's front panel (it's actually a screen showing a camera view of the eyes, but based on Apple's images you might be convinced it's a simple plane of glass). If they're fully immersed in an experience, onlookers will instead see a cloud of color to signify that they're exploring another world.\n",
      "As the rumors had suggested, the Apple Vision Pro headset will come with some impressive specs to justify its sky-high price.\n",
      "First, the Vision Pro will use two chipsets to power its experiences. One is an M2 chip, the same one you'll find in the Apple iPad Pro (2022), and some of the best MacBooks and Macs. This powerful processor will handle the apps and software you're running on the Vision Pro. Meanwhile, the R1 chipset will deal with the mixed reality side of things, processing the immersive elements of the Vision Pro that turn it from a glorified wearable Mac display to an immersive \"spatial computer\".\n",
      "\n",
      "On top of these chips, the Vision Pro has crisp 4K micro-OLED displays – one per eye – that offer roughly 23 million pixels each. According to Apple the Vision Pro's display fits 64 pixels into the same space that the iPhone's screen fits one single pixel, and this could eliminate the annoying screen-door effect that affects other VR headsets. This effect occurs when you're up close to a screen and you can start to see the gaps between the pixels in the array; the higher the pixel density, the closer you can get before the screen door effect becomes noticeable.\n",
      "These components will allow you to run an array of Apple software through Apple's new visionOS platform (not xrOS as was rumored). This includes immersive photos and videos, custom-made Disney Plus experiences, and productivity apps like Keynote.\n",
      "You'll also be able to play over 100 Apple Arcade titles on a virtual screen that's like your own private movie theatre.\n",
      "\n",
      "You'll be able to connect your Vision Pro headset to a Mac via Bluetooth. When using this feature you'll be able to access your Mac apps and see your screen on a large immersive display, and it'll sit alongside other Vision Pro apps you're using. Apple says this setup will help you be more productive than you've ever been.\n",
      "With the power of the M2 chip, Apple's headset should be able to run most Mac apps natively – Final Cut Pro and Logic Pro recently arrived on M2 iPads. For now, however, Apple hasn't revealed if these and other apps will be available natively on the Vision Pro, or if you'll need a Mac to unlock the headset's full potential. We expect these details will be revealed nearer to the headset's 2024 launch.\n",
      "Sign up to receive daily breaking news, reviews, opinion, analysis, deals and more from the world of tech.\n",
      "Hamish is a Staff Writer for TechRadar and you’ll see his name appearing on articles across nearly every topic on the site from smart home deals to speaker reviews to graphics card news and everything in between. He uses his broad range of knowledge to help explain the latest gadgets and if they’re a must-buy or a fad fueled by hype. Though his specialty is writing about everything going on in the world of virtual reality and augmented reality.\n",
      "I am so ready to play and work in Apple Vision Pro\n",
      "Apple Vision Pro is the best gadget I've seen in ages but it still needs a purpose\n",
      "New Atlus RPG Metaphor ReFantazio announced at Xbox Games Showcase\n",
      "By Cat BussellJune 11, 2023\n",
      "By Aleksha McLoughlinJune 11, 2023\n",
      "By Cat BussellJune 11, 2023\n",
      "By Rhys WoodJune 11, 2023\n",
      "By Aleksha McLoughlinJune 11, 2023\n",
      "By Cat BussellJune 11, 2023\n",
      "By Aleksha McLoughlinJune 11, 2023\n",
      "By Aleksha McLoughlinJune 11, 2023\n",
      "By David NieldJune 11, 2023\n",
      "By Marc McLarenJune 11, 2023\n",
      "By Elie GouldJune 10, 2023\n",
      "TechRadar is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site.\n",
      "©\n",
      "Future US, Inc. Full 7th Floor, 130 West 42nd Street,\n",
      "New York,\n",
      "NY 10036.\n",
      "[2023-06-13 21:20:36,763] Browsing:INFO - sufficient_or_not_json: {'is_sufficient': True}\n",
      "[2023-06-13 21:20:36,763] Browsing:INFO - Feedback bot decided to finish browsing.\n",
      "[2023-06-13 21:20:36,764] Browsing:INFO - We've got the answer!\n"
     ]
    }
   ],
   "source": [
    "browsing_result = (\n",
    "    await full_browsing(\n",
    "        query=query,\n",
    "        tokens_per_chunk=2048,\n",
    "        chunk_overlap=1024,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Vision Pro is Apple's first AR and VR headset, featuring a mixed reality headset design with dual M2 and R1 chip setup, 4K resolution per eye, no controllers, external battery pack, and two-hour battery life. It uses hand tracking and voice inputs to control its visionOS software. The headset is expected to release sometime \"early next year\" and starts at $3,499. It relies solely on tracking hand and eye movements as well as voice inputs to control its apps and experiences. The headset uses an array of 12 cameras, five sensors, and six microphones to pick up these inputs. The headset uses two chipsets, an M2 and a new R1 to handle regular software and its XR capabilities, respectively. The Vision Pro is expected to run most Mac apps natively, and users will be able to access their Mac apps and see their screen on a large immersive display when connected to a Mac via Bluetooth.\n"
     ]
    }
   ],
   "source": [
    "llm = Shared().openai_llm\n",
    "llm_output = await llm.apredict(  # type: ignore\n",
    "    QueryTemplates.CONTEXT_QUESTION__DEFAULT.format(\n",
    "        context=browsing_result, question=query\n",
    "    )\n",
    ")\n",
    "print(llm_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
